# Big-data-hadoop
The project has basically 3 parts -

->We created our own cluster in our lab with 7-8 computers , in which one was Name Node and the others were Data Node .
  1. we first implemented HDFS . uploaded a file of around 4 GB in our cluster.
  2. we perform Map reduce . basically we run a program for word count on that file .
  3. we implemeted hive in which we created a database and perform ceratin operation in it.

->We created our cluster using AWS instances . // at run time it asks user about the no of instances to launch and then make one of those instances Namenode and othe datanode.

->We created our cluster using DOCKER.

# Screenshots of my project
## First login 

![screenshot from 2018-07-14 18-43-37](https://user-images.githubusercontent.com/20671151/42725439-059591d4-87a1-11e8-9078-f41e58ba47a2.png)

## After a successful login you have the three options as described above 

